# -*- coding: utf-8 -*-
"""Spam_Filter_by_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mk100qv7ha3EyiM159xv2B9ff-kZ0vLf
"""

import pandas as pd
import numpy as np
import random
import os
import nltk
import pickle
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import re
from google.colab import files
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.regularizers import l2

!wget https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv
spam = pd.read_csv("spam.csv", encoding='ISO-8859-1')
spam.head()

spam.isnull().sum()

spam = spam[['v1', 'v2']]
spam.columns = ['lable', 'msg']
spam.head()

nltk.download('stopwords')
stop_words = stopwords.words('english')

ps = PorterStemmer()

def clean_msg(text):
  msg = re.sub('[^a-zA-Z]' , ' ' , text)
  msg = msg.lower()
  msg = msg.split()
  msgs = [ps.stem(word) for word in msg if not word in stop_words]
  return " ".join(msgs)

spam.loc[:, 'msg'] = spam['msg'].apply(clean_msg)
spam.head()

spam['lable'] = spam['lable'].map({'ham': 0, 'spam': 1})

X = spam['msg'].values
y = spam['lable'].values

print("Labels check (First 5):", y[:5])

vocab_size = 10000
max_length = 100
embedding_dim = 16
trunc_type = 'post'
padding_type = 'post'
oov_tok = "<OOV>"

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)

tokenizer.fit_on_texts(X_train)

word_index = tokenizer.word_index
print(f"Total Unique Words found: {len(word_index)}")

training_sequences = tokenizer.texts_to_sequences(X_train)
testing_sequences = tokenizer.texts_to_sequences(X_test)


training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)


training_padded = np.array(training_padded)
training_labels = np.array(y_train)
testing_padded = np.array(testing_padded)
testing_labels = np.array(y_test)

print("\n--- Data Ready for Model ---")
print(f"Training Data Shape: {training_padded.shape}")
print(f"Testing Data Shape: {testing_padded.shape}")

def set_seed(seed=42):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    random.seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'

set_seed(42)
tf.keras.backend.clear_session()

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(0.01))),
    tf.keras.layers.Dropout(0.6),
    tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.01))
])

custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
model.compile(loss='binary_crossentropy', optimizer=custom_optimizer, metrics=['accuracy'])

history = model.fit(
    training_padded, training_labels,
    epochs=20,
    validation_data=(testing_padded, testing_labels),
    verbose=2
)

loss, accuracy = model.evaluate(testing_padded, testing_labels)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

y_pred_prob = model.predict(testing_padded)
y_pred_standard = (y_pred_prob > 0.32).astype("int32")

print("\n--- Confusion Matrix (Normal 0.32) ---")
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(testing_labels, y_pred_standard)
print(cm)

model.save('Final_Spam_Model.keras')
print("Model saved as 'Final_Spam_Model.keras' ✅")

with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
print("Tokenizer saved as 'tokenizer.pickle' ✅")

print("Downloading files...")
files.download('Final_Spam_Model.keras')
files.download('tokenizer.pickle')